# -*- coding: utf-8 -*-
"""
Created on Wed Sep 14 21:01:54 2022

@author: marcnetts
"""

from linear_algebra import dot
import random
import matplotlib.pyplot as plt

def degrau(x):
    return 1 if x >= 0 else 0 # 0 é o limial

def saida_perceptron(pesos, entradas):
        y = dot(pesos, entradas)
        return degrau(y)

def ajustes(sinapses, entradas, saida):
    taxa_aprendizagem = 0.1
    saida_parcial = saida_perceptron(sinapses, entradas)
    
    for j in range(3):
        erro = saida - saida_parcial
        sinapses[j] = sinapses[j] + taxa_aprendizagem * erro * entradas[j]
        
    saida = saida_parcial
    return sinapses, saida

def teste_generalizacao(sinapses, entradas, saida):
    saida_parcial = saida_perceptron(sinapses, entradas)
    saida = saida_parcial
    return sinapses, saida

neuronio = [0.22, -0.33, 0.44]

padroes = [
	[-1, 0.962460897, 1.000000, 1], 
	[-1, 0.984358707, 0.771450, 1], 
	[-1, 0.86548488, 0.737828, 1], 
	[-1, 0.739311783, 0.597097, 0], 
	[-1, 0.790406674, 0.596730, 0], 
	[-1, 0.96350365, 0.537387, 1], 
	[-1, 0.936392075, 0.524711, 1], 
	[-1, 0.788321168, 0.452508, 0], 
	[-1, 0.927007299, 0.425684, 1], 
	[-1, 0.868613139, 0.401617, 1], 
	[-1, 0.961418144, 0.373875, 1], 
	[-1, 0.937434828, 0.326658, 1], 
	[-1, 0.907194995, 0.326107, 1], 
	[-1, 0.97080292, 0.325923, 1], 
	[-1, 0.987486966, 0.229285, 1], 
	[-1, 0.8362878, 0.227081, 0], 
	[-1, 0.557872784, 0.205953, 0], 
	[-1, 0.566214807, 0.194929, 0], 
	[-1, 0.840458811, 0.171468, 1], 
	[-1, 0.979144943, 0.169833, 1], 
	[-1, 0.875912409, 0.169410, 1], 
	[-1, 0.740354536, 0.161161, 0], 
	[-1, 0.758081335, 0.140952, 0], 
	[-1, 0.788321168, 0.130737, 0], 
	[-1, 0.708029197, 0.119291, 0], 
	[-1, 0.765380605, 0.115745, 0], 
	[-1, 0.967674661, 0.097226, 1], 
	[-1, 0.979144943, 0.097024, 1], 
	[-1, 0.996871741, 0.096105, 1], 
	[-1, 0.982273201, 0.095168, 1], 
	[-1, 0.989572471, 0.088279, 1], 
	[-1, 0.848800834, 0.087929, 1], 
	[-1, 0.888425443, 0.083061, 1], 
	[-1, 0.952033368, 0.081021, 1], 
	[-1, 0.794577685, 0.079056, 0], 
	[-1, 0.724713243, 0.070108, 0], 
	[-1, 1, 0.070035, 1], 
	[-1, 0.808133472, 0.067904, 0], 
	[-1, 0.983315954, 0.064909, 1], 
	[-1, 0.890510949, 0.062374, 1], 
	[-1, 0.95620438, 0.058258, 1], 
	[-1, 0.899895725, 0.057707, 1], 
	[-1, 0.923879041, 0.054988, 1], 
	[-1, 0.708029197, 0.054859, 0], 
	[-1, 0.857142857, 0.053224, 1], 
	[-1, 0.885297185, 0.053206, 1], 
	[-1, 0.987486966, 0.052875, 1], 
	[-1, 0.97810219, 0.044994, 1], 
	[-1, 0.519290928, 0.036855, 0], 
	[-1, 0.615224192, 0.035569, 0], 
	[-1, 0.762252346, 0.035458, 0], 
	[-1, 0.862356621, 0.034926, 1], 
	[-1, 0.976016684, 0.034723, 1], 
	[-1, 0.883211679, 0.033015, 1], 
	[-1, 0.531803962, 0.032592, 0], 
	[-1, 0.796663191, 0.031784, 0], 
	[-1, 0.761209593, 0.031637, 0], 
	[-1, 0.571428571, 0.029855, 0], 
	[-1, 0.8362878, 0.028201, 1], 
	[-1, 0.662148071, 0.025372, 0], 
	[-1, 0.768508863, 0.025299, 0], 
	[-1, 0.659019812, 0.024619, 0], 
	[-1, 0.838373306, 0.019419, 1], 
	[-1, 0.835245047, 0.019125, 1], 
	[-1, 0.891553702, 0.018758, 1], 
	[-1, 0.916579771, 0.016805, 1], 
	[-1, 0.602711157, 0.016450, 0], 
	[-1, 0.546402503, 0.016386, 0], 
	[-1, 0.753910323, 0.016351, 0], 
	[-1, 0.761209593, 0.016335, 0], 
	[-1, 0.757038582, 0.016213, 0], 
	[-1, 0.85088634, 0.015422, 1], 
	[-1, 0.72367049, 0.015381, 0], 
	[-1, 0.629822732, 0.014549, 0], 
	[-1, 0.856100104, 0.014360, 1], 
	[-1, 0.503649635, 0.012760, 0], 
	[-1, 0.594369135, 0.012664, 0], 
	[-1, 0.621480709, 0.011797, 0], 
	[-1, 0.96350365, 0.011411, 1], 
	[-1, 0.733055266, 0.011385, 0], 
	[-1, 0.534932221, 0.010068, 0], 
	[-1, 0.601668405, 0.009238, 0], 
	[-1, 0.633993743, 0.009065, 0], 
	[-1, 0.647549531, 0.008506, 0], 
	[-1, 0.81438999, 0.008236, 0], 
	[-1, 0.776850886, 0.007999, 0], 
	[-1, 0.445255474, 0.007573, 0], 
	[-1, 0.8362878, 0.007321, 1], 
	[-1, 0.472367049, 0.006814, 0], 
	[-1, 0.740354536, 0.006735, 0], 
	[-1, 0.681960375, 0.006687, 0], 
	[-1, 0.827945777, 0.006616, 0], 
	[-1, 0.86548488, 0.006223, 1], 
	[-1, 0.624608968, 0.005791, 0], 
	[-1, 0.583941606, 0.005547, 0], 
	[-1, 0.413972888, 0.005258, 0], 
	[-1, 0.485922836, 0.005139, 0], 
	[-1, 0.660062565, 0.004887, 0], 
	[-1, 0.743482795, 0.004788, 0], 
	[-1, 0.546402503, 0.004665, 0], 
	[-1, 0.558915537, 0.004534, 0], 
	[-1, 0.798748697, 0.004358, 0], 
	[-1, 0.4181439, 0.004016, 0], 
	[-1, 0.402502607, 0.003676, 0], 
	[-1, 0.563086548, 0.003669, 0], 
	[-1, 0.949947862, 0.003538, 1], 
	[-1, 0.997914494, 0.003340, 1], 
	[-1, 0.579770594, 0.003175, 0], 
	[-1, 0.557872784, 0.002383, 0], 
	[-1, 0.495307612, 0.002122, 0], 
	[-1, 0.861313869, 0.002036, 1], 
	[-1, 0.515119917, 0.001727, 0], 
	[-1, 0.774765381, 0.001597, 0], 
	[-1, 0.768508863, 0.001586, 0], 
	[-1, 0.696558916, 0.001323, 0], 
	[-1, 0.765380605, 0.001268, 0], 
	[-1, 0.543274244, 0.001224, 0], 
	[-1, 0.751824818, 0.001157, 0], 
	[-1, 0.500521376, 0.001123, 0], 
	[-1, 0.522419187, 0.001021, 0], 
	[-1, 0.82168926, 0.000959, 1], 
	[-1, 0.690302398, 0.000694, 0], 
	[-1, 0.424400417, 0.000623, 0], 
	[-1, 0.884254432, 0.000612, 1], 
	[-1, 0.719499479, 0.000592, 0], 
	[-1, 0.503649635, 0.000582, 0], 
	[-1, 0.753910323, 0.000467, 0], 
	[-1, 0.825860271, 0.000299, 0], 
	[-1, 0.58915537, 0.000244, 0], 
	[-1, 0.586027112, 0.000243, 0], 
	[-1, 0.796663191, 0.000231, 0], 
	[-1, 0.742440042, 0.000209, 0], 
	[-1, 0.633993743, 0.000141, 0], 
	[-1, 0.645464025, 0.000127, 0], 
	[-1, 0.776850886, 0.000108, 0], 
	[-1, 0.806047967, 0.000048, 1], 
	[-1, 0.666319082, 0.000007, 0]
]

#23 ciclos de treinamento
for n in range(23):
    print("Ciclo " + str(n+1))
    #neuronio, saida_1 = ajustes(neuronio, padrao_0[0], saida1)
    for i in range(len(padroes)):
        neuronio, saida_1 = ajustes(neuronio, padroes[i][:-1], padroes[i][2])
        print(neuronio, "saida0 = ", saida_1)

### Plotting

plt.title("Separação de classes com Perceptron")
plt.xlabel("IDH")
plt.ylabel("PIB")
# =============================================================================
# x = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]
# y = [-0.125, -0.0125, 0.1, 0.2125, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875]
# plt.plot(y, x, color = 'green', marker = '*', linestyle = '--')
# =============================================================================
for padrao in padroes:
    print(padrao[1])
    print(padrao[2])
    corEsperada = "purple" if padrao[3] == 0 else "blue"
    plt.scatter(padrao[1], padrao[2], c=corEsperada)
#    plt.scatter(padrao[1], padrao[2], c="#2ca02c")



### Testes
print("Testes de generalização")
padroes_testes_0 = [
    [-1, 0.2, 0.4],
    [-1, 0.6, 0.3],
    [-1, 0.2, 0.6],
    ]
padroes_testes_1 = [
    [-1, 0.7, 0.8],
    [-1, 0.1, 0.9],
    [-1, 0.8, 0.1],
    ]
saida0 = [0]
saida1 = [1]
# =============================================================================
# for teste in padroes_testes_0:
#     neuronio, saida_0 = teste_generalizacao(neuronio, teste, saida0)
#     print(neuronio, teste, "saida0 = ", saida_0)
#     corEsperada = "purple" if saida_0 == 0 else "red"
#     plt.scatter(teste[1], teste[2], c=corEsperada)
# for teste in padroes_testes_1:
#     neuronio, saida_1 = teste_generalizacao(neuronio, teste, saida1)
#     print(neuronio, teste, "saida1 = ", saida_1)
#     corEsperada = "purple" if saida_1 == 1 else "red"
#     plt.scatter(teste[1], teste[2], c=corEsperada)
# =============================================================================


# O valor [-1, 0.6, 0.3] será 1 no teste, mesmo que o esperado seja 0;
# Ele está fora dos valores padrões que foram inseridos para treinar o modelo, um outlier da reta esperada

